{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b', 2), ('g', 3), ('i', 4), ('s', 3), ('y', 2), ('r', 2), ('l', 3), ('h', 3), ('d', 1), ('e', 6), ('v', 1), ('n', 4), ('m', 3), ('a', 3), ('x', 1), ('o', 3), ('u', 1), ('t', 2), ('w', 1)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def init_spark():\n",
    "  spark = SparkSession.builder.appName(\"HelloWorld\").getOrCreate()\n",
    "  sc = spark.sparkContext\n",
    "  return spark,sc\n",
    "\n",
    "def main():\n",
    "  spark,sc = init_spark()\n",
    "  input_file = sc.textFile(\"input.txt\")\n",
    "  map = input_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1))\n",
    "  counts = map.reduceByKey(lambda a, b: a + b)\n",
    "  counts=counts.sortByKey()\n",
    "  # filter out words with fewer than threshold occurrences\n",
    "  filtered = counts.filter(lambda pair:pair[1] >= 1)\n",
    "\n",
    "  # count characters\n",
    "  charCounts = filtered.flatMap(lambda pair:pair[0]).map(lambda c: c).map(lambda c: (c, 1)).reduceByKey(lambda v1,v2:v1 +v2)\n",
    "\n",
    "  list = charCounts.collect()\n",
    "  print(repr(list)[1:-1])\n",
    "  counts.saveAsTextFile(\"output3\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x00000277FFD57898>\n",
      "Partitions structure: [[], [(('United Kingdom', 'Bob'), 100), (('United Kingdom', 'Bob'), 15), (('Poland', 'Marek'), 51)], [(('Poland', 'Paul'), 75)], [(('Germany', 'Johannes'), 200)]]\n",
      "[(('Germany', 'Johannes'), 200), (('Poland', 'Marek'), 51), (('Poland', 'Paul'), 75), (('United Kingdom', 'Bob'), '100,15')]\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def init_spark():\n",
    "  spark = SparkSession.builder.appName(\"HelloWorld\").getOrCreate()\n",
    "  sc = spark.sparkContext\n",
    "  return spark,sc\n",
    "\n",
    "def main():\n",
    "  spark,sc = init_spark()\n",
    "  rdd = sc.parallelize(transactions) \\\n",
    "        .map(lambda el: ((el['country'], el['name']),el['amount'])) \\\n",
    "        .partitionBy(4, country_partitioner)\n",
    "  print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "  print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "  print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "  counts = rdd.reduceByKey(lambda a, b: str(a)+','+ str( b))\n",
    "  counts=counts.sortByKey()\n",
    "  print(counts.collect())\n",
    "def country_partitioner(country):\n",
    "    return hash(country)  \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'},\n",
    "    {'name': 'Bob', 'amount': 15, 'country': 'United Kingdom'},\n",
    "    {'name': 'Marek', 'amount': 51, 'country': 'Poland'},\n",
    "    {'name': 'Johannes', 'amount': 200, 'country': 'Germany'},\n",
    "    {'name': 'Paul', 'amount': 75, 'country': 'Poland'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "keyedElems = input.map(lambda x: (x[0], (x[1], x[2])))\n",
    "groupedCombinations = keyedElems.groupByKey().flatMapValues(lambda arr: itertools.combinations(arr, 2))\n",
    "productScoreCombinations = groupedCombinations.mapValues(lambda elems: ((elems[0][0], elems[1][0]), (elems[0][1], elems[1][1]))).map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
